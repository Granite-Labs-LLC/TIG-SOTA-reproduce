/*!
Copyright 2025 Granite Labs LLC

Identity of Submitter [name of person or entity that submits the Work to TIG]

Licensed under the TIG Inbound Game License v2.0 or (at your option) any later
version (the "License"); you may not use this file except in compliance with the
License. You may obtain a copy of the License at

https://github.com/tig-foundation/tig-monorepo/tree/main/docs/licenses

Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the specific
language governing permissions and limitations under the License.
*/

// TIG's UI uses the pattern `tig_challenges::<challenge_name>` to automatically detect your algorithm's challenge

// when launching kernels, you should not exceed this const or else it may not be deterministic
const MAX_THREADS_PER_BLOCK: u32 = 1024;

//
// stat_filter
//
// Filtering based on Median Absolute Deviation (MAD):
// We compute the median of all L2 norms, then calculate the MAD (median of
// absolute deviations from the median). The threshold is set to:
//      norm_threshold = scale_factor × MAD × 1.4826
// The factor 1.4826 scales MAD to match the standard deviation for normally
// distributed data. This makes the filter more robust to outliers compared to
// filtering methods based on mean and standard deviation, which are more
// sensitive to extreme values.
//
// Reference:
// - NIST Engineering Statistics Handbook:
//   https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm
// - See also: https://www.itl.nist.gov/div898/handbook/eda/section3/eda356.htm
//

//use crate::{seeded_hasher, HashMap, HashSet};
use std::sync::Arc;
use anyhow::{anyhow, Result};
use cudarc::{
    driver::{CudaModule, CudaStream, LaunchConfig, PushKernelArg},
    runtime::sys::cudaDeviceProp,
};

use tig_challenges::vector_search::*;
use std::env;


const MAD_SCALE_NORMAL: f32 = 1.4826;

pub fn solve_challenge(
    challenge: &Challenge,
    module: Arc<CudaModule>,
    stream: Arc<CudaStream>,
    prop: &cudaDeviceProp,
) -> Result<Option<Solution>> {
    let start_time_total = std::time::Instant::now();

    // Load kernels
    let compute_vector_stats_kernel  = module.load_function("compute_vector_stats_kernel")?;
    let find_nearest_neighbor_kernel = module.load_function("find_nearest_neighbor_kernel")?;

    // Allocations
    let d_norm_l2 = stream.alloc_zeros::<f32>(challenge.database_size as usize)?;
    let d_norm_l2_squared = stream.alloc_zeros::<f32>(challenge.database_size as usize)?;
    let mut d_query_results = stream.alloc_zeros::<u32>(challenge.difficulty.num_queries as usize)?;

    // Launch kernel 1: compute_vector_stats_kernel
    let threads_per_block = prop.maxThreadsPerBlock as u32;
    let num_blocks = (challenge.database_size + threads_per_block - 1) / threads_per_block;

    let shared_mem_bytes1 = (prop.maxThreadsPerBlock as usize * 2 * std::mem::size_of::<f64>()) as u32;

    let cfg1 = LaunchConfig {
        grid_dim: (num_blocks, 1, 1),
        block_dim: (threads_per_block, 1, 1),
        shared_mem_bytes: shared_mem_bytes1,
    };

    unsafe {
        stream
            .launch_builder(&compute_vector_stats_kernel)
            .arg(&challenge.d_database_vectors)
            .arg(&d_norm_l2)
            .arg(&d_norm_l2_squared)
            .arg(&challenge.database_size)
            .arg(&challenge.vector_dims)
            .launch(cfg1)?;
    }

    stream.synchronize()?;

    let elapsed_time_ms_1 = start_time_total.elapsed().as_micros() as f32 / 1000.0;

    // Copy norms to host and compute median + MAD
    let mut h_norms = stream.memcpy_dtov(&d_norm_l2)?;
    h_norms.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let mid = h_norms.len() / 2;
    let median = if h_norms.len() % 2 == 0 {
        (h_norms[mid - 1] + h_norms[mid]) / 2.0
    } else {
        h_norms[mid]
    };

    let mut deviations: Vec<f32> = h_norms
        .iter()
        .map(|&x| (x - median).abs())
        .collect();
    deviations.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let mad = if deviations.len() % 2 == 0 {
        (deviations[mid - 1] + deviations[mid]) / 2.0
    } else {
        deviations[mid]
    };

    let scale = env::var("SCALE_OVERRIDE")
    .ok()
    .and_then(|v| v.parse::<f32>().ok())
    .unwrap_or_else(|| scale_factor(challenge.difficulty.num_queries as usize));
    println!("stat_filter scale: {}", scale);

    let norm_threshold = scale * mad * MAD_SCALE_NORMAL;

    let elapsed_time_ms_2 = start_time_total.elapsed().as_micros() as f32 / 1000.0;

    // Launch kernel 2: find_nearest_neighbor_kernel
    let shared_mem_bytes2 = (challenge.vector_dims as usize * std::mem::size_of::<f32>()) as u32
        + (prop.maxThreadsPerBlock as usize * std::mem::size_of::<f32>()) as u32
        + (prop.maxThreadsPerBlock as usize * std::mem::size_of::<usize>()) as u32;

    let cfg2 = LaunchConfig {
        grid_dim: (challenge.difficulty.num_queries, 1, 1),
        block_dim: (threads_per_block, 1, 1),
        shared_mem_bytes: shared_mem_bytes2,
    };

    unsafe {
        stream
            .launch_builder(&find_nearest_neighbor_kernel)
            .arg(&challenge.d_query_vectors)
            .arg(&challenge.d_database_vectors)
            .arg(&d_norm_l2)
            .arg(&d_norm_l2_squared)
            .arg(&mut d_query_results)
            .arg(&challenge.max_distance)
            .arg(&challenge.database_size)
            .arg(&challenge.difficulty.num_queries)
            .arg(&challenge.vector_dims)
            .arg(&norm_threshold)
            .launch(cfg2)?;
    }

    stream.synchronize()?;


    let raw_indexes: Vec<u32> = stream.memcpy_dtov(&d_query_results)?;
    let indexes: Vec<usize> = raw_indexes.into_iter().map(|i| i as usize).collect();

    let elapsed_time_ms = start_time_total.elapsed().as_micros() as f32 / 1000.0;
    println!(
        "Time for nonce: {} ms ({} ms + {} ms + {} ms)",
        elapsed_time_ms,
        elapsed_time_ms_1,
        elapsed_time_ms_2 - elapsed_time_ms_1,
        elapsed_time_ms - elapsed_time_ms_2
    );

    Ok(Some(Solution { indexes }))
}

// Tune the scale for various query set sizes
fn scale_factor(num_queries: usize) -> f32 {
    match num_queries {
        q if q <= 700 => 0.20,
        q if q <= 1000 => 0.20 + (q as f32 - 700.0) * (0.10 / 300.0),       // 0.30 at 1000
        q if q <= 1500 => 0.30 + (q as f32 - 1000.0) * (0.20 / 500.0),      // 0.50 at 1500
        q if q <= 2000 => 0.50 + (q as f32 - 1500.0) * (0.44 / 500.0),      // 0.94 at 2000
        q if q <= 2500 => 0.94 + (q as f32 - 2000.0) * (1.08 / 500.0),      // 2.02 at 2500
        //_ => 2.02,
        _ => 1.00,
        //_ => 0.50,
        //_ => 0.25,
        //_ => 0.10,
        //_ => 0.01,
        //_ => 0.0001,
    }
}

